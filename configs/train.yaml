# Training configuration (flattened for trainer)
epochs: 60
batch_size: 16
num_workers: 4
pin_memory: true
seed: 42
debug_mode: false

# Optimizer (trainer expects 'type' and 'learning_rate')
optimizer:
  type: "adam"
  learning_rate: 1e-4
  weight_decay: 1e-4
  betas: [0.9, 0.999]

# Learning rate scheduler
scheduler:
  type: "cosine"
  T_max: 100
  eta_min: 1e-6

  # fallback params (unused for cosine)
  step_size: 30
  gamma: 0.1

  # plateau params (unused here)
  mode: "max"
  patience: 10
  factor: 0.5
  threshold: 0.001

# Loss function (top-level block)
loss:
  loss_type: "combined"   # combined, dice, focal, iou
  dice_weight: 0.6
  focal_weight: 0.4
  aux_weight: 0.3
  focal_alpha: 0.75
  focal_gamma: 2.0
  dice_smooth: 1.0
  iou_smooth: 1.0
  class_weights: [1.0, 5.0]
  reduction: "mean"

# Data augmentation (informational - used by dataset)
augmentation:
  probability: 0.8
  transforms:
    horizontal_flip: 0.5
    vertical_flip: 0.5
    rotation: 0.3
    brightness_contrast: 0.3
    gaussian_noise: 0.2
    cutout: 0.2
  rotation_limit: 15
  brightness_limit: 0.2
  contrast_limit: 0.2
  noise_var_limit: [0.0, 0.05]
  cutout_holes: 8
  cutout_size: 32

# Validation and monitoring
validation_interval: 1            # validate every N epochs
validation:
  metrics: ["iou", "dice", "precision", "recall", "f1"]
  threshold: 0.5
  early_visualize: true

# Early stopping
early_stopping:
  patience: 15
  monitor: "val_fire_f1"
  mode: "max"
  min_delta: 0.001
  restore_best_weights: true

# Checkpointing (informational - trainer uses save_interval and save_best logic)
checkpoint:
  save_top_k: 3
  monitor: "val_fire_f1"
  mode: "max"
  save_last: true
  dirpath: "checkpoints/"
  filename: "unet-{epoch:02d}-{val_fire_f1:.4f}"

# Logging
logging:
  log_every_n_steps: 50
  save_dir: "logs/"
  name: "fire_detection"
  project: "forest_fire_unet"
  wandb:
    enabled: false
    project: "forest-fire-detection"
    entity: null
    tags: ["unet", "fire-segmentation", "phase1"]

# Precision / mixed precision
precision: 32
mixed_precision: false

# Gradient clipping (trainer expects this key name)
gradient_clipping: 1.0

# Trainer tuning
log_interval: 10
validation_frequency: 1
validation_interval: 1   # keep this to match trainer usage
save_interval: 5

# Misc (kept for downstream use)
tile_size: 512
tile_overlap: 64
